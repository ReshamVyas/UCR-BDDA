{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HADOOP COMMANDS\n",
    "### 1. STARTING HADOOP\n",
    "##### allstart.sh\n",
    "### 2. COPYING THE CORONA FILE FROM LOCAL TO HADOOP\n",
    "#### hadoop fs -copyFromLocal Corona_NLP_train.csv\n",
    "### 3. OPENING JUPYTER NOTEBOOK \n",
    "##### pysparknb\n",
    "\n",
    "## Resham Vyas\n",
    "## 015028"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler, StandardScaler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"Corona_NLP_train.csv\", sep=\",\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|            UserName|          ScreenName|            Location|             TweetAt|       OriginalTweet|Sentiment|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|                3799|               48751|              London|          16-03-2020|@MeNyrbie @Phil_G...|  Neutral|\n",
      "|                3800|               48752|                  UK|          16-03-2020|advice Talk to yo...| Positive|\n",
      "|                3801|               48753|           Vagabonds|          16-03-2020|Coronavirus Austr...| Positive|\n",
      "|                3802|               48754|                null|          16-03-2020|My food stock is ...|     null|\n",
      "|              PLEASE|         don't panic| THERE WILL BE EN...|                null|                null|     null|\n",
      "|           Stay calm|          stay safe.|                null|                null|                null|     null|\n",
      "|#COVID19france #C...|            Positive|                null|                null|                null|     null|\n",
      "|                3803|               48755|                null|          16-03-2020|Me, ready to go a...|     null|\n",
      "|Not because I'm p...| but because my f...|          but please| don't panic. It ...|                null|     null|\n",
      "|#CoronavirusFranc...|  Extremely Negative|                null|                null|                null|     null|\n",
      "|                3804|               48756|ÜT: 36.319708,-82...|          16-03-2020|As news of the re...| Positive|\n",
      "|                3805|               48757|35.926541,-78.753267|          16-03-2020|\"Cashier at groce...| Positive|\n",
      "|                3806|               48758|             Austria|          16-03-2020|Was at the superm...|     null|\n",
      "|#toiletpapercrisi...|             Neutral|                null|                null|                null|     null|\n",
      "|                3807|               48759|     Atlanta, GA USA|          16-03-2020|Due to COVID-19 o...| Positive|\n",
      "|                3808|               48760|    BHAVNAGAR,GUJRAT|          16-03-2020|For corona preven...| Negative|\n",
      "|                3809|               48761|      Makati, Manila|          16-03-2020|All month there h...|  Neutral|\n",
      "|                3810|               48762|Pitt Meadows, BC,...|          16-03-2020|Due to the Covid-...|     null|\n",
      "|The wait time may...| particularly bee...|                null|                null|                null|     null|\n",
      "|We thank you for ...|  Extremely Positive|                null|                null|                null|     null|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet', 'Sentiment']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserName: string (nullable = true)\n",
      " |-- ScreenName: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- TweetAt: string (nullable = true)\n",
      " |-- OriginalTweet: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(UserName='3799', ScreenName='48751', Location='London', TweetAt='16-03-2020', OriginalTweet='@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8', Sentiment='Neutral')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(UserName='44954', ScreenName='89906', Location=None, TweetAt='14-04-2020', OriginalTweet='Is it wrong that the smell of hand sanitizer is starting to turn me on?', Sentiment=None),\n",
       " Row(UserName='#coronavirus #COVID19 #coronavirus\"', ScreenName='Neutral', Location=None, TweetAt=None, OriginalTweet=None, Sentiment=None),\n",
       " Row(UserName='44955', ScreenName='89907', Location='i love you so much || he/him', TweetAt='14-04-2020', OriginalTweet=\"@TartiiCat Well new/used Rift S are going for $700.00 on Amazon rn although the normal market price is usually $400.00 . Prices are really crazy right now for vr headsets since HL Alex was announced and it's only been worse with COVID-19. Up to you whethe\", Sentiment='Negative')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68046"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+--------------------+------------+--------------------+--------------------+\n",
      "|summary|            UserName|         ScreenName|            Location|     TweetAt|       OriginalTweet|           Sentiment|\n",
      "+-------+--------------------+-------------------+--------------------+------------+--------------------+--------------------+\n",
      "|  count|               68042|              55629|               34247|       41735|               41383|               28617|\n",
      "|   mean| 8.975066046523207E7| 150387.47623557984|    1.76800437504E10|        10.0|               682.0|              1016.0|\n",
      "| stddev|  9.10012357766514E9|1.645384113616883E7|8.839999088543759E10|         NaN|  1176.0629234866644|  1418.4562030602144|\n",
      "|    min|                 ...|                   |                    |            |      Coronavirus...| \"\" Well covid-19...|\n",
      "|    max|Ø  With April 1 d...|         but for us|ï? ???????'? ????...| she says.\"|Ø  As buyers stoc...| when I also rece...|\n",
      "+-------+--------------------+-------------------+--------------------+------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|             TweetAt|       OriginalTweet|Sentiment|\n",
      "+--------------------+--------------------+---------+\n",
      "|          16-03-2020|@MeNyrbie @Phil_G...|  Neutral|\n",
      "|          16-03-2020|advice Talk to yo...| Positive|\n",
      "|          16-03-2020|Coronavirus Austr...| Positive|\n",
      "|          16-03-2020|My food stock is ...|     null|\n",
      "|                null|                null|     null|\n",
      "|                null|                null|     null|\n",
      "|                null|                null|     null|\n",
      "|          16-03-2020|Me, ready to go a...|     null|\n",
      "| don't panic. It ...|                null|     null|\n",
      "|                null|                null|     null|\n",
      "|          16-03-2020|As news of the re...| Positive|\n",
      "|          16-03-2020|\"Cashier at groce...| Positive|\n",
      "|          16-03-2020|Was at the superm...|     null|\n",
      "|                null|                null|     null|\n",
      "|          16-03-2020|Due to COVID-19 o...| Positive|\n",
      "|          16-03-2020|For corona preven...| Negative|\n",
      "|          16-03-2020|All month there h...|  Neutral|\n",
      "|          16-03-2020|Due to the Covid-...|     null|\n",
      "|                null|                null|     null|\n",
      "|                null|                null|     null|\n",
      "+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('TweetAt','OriginalTweet','Sentiment').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('TweetAt','OriginalTweet','Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26663"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()['OriginalTweet'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=('OriginalTweet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|   TweetAt|       OriginalTweet|         Sentiment|\n",
      "+----------+--------------------+------------------+\n",
      "|16-03-2020|@MeNyrbie @Phil_G...|           Neutral|\n",
      "|16-03-2020|advice Talk to yo...|          Positive|\n",
      "|16-03-2020|Coronavirus Austr...|          Positive|\n",
      "|16-03-2020|My food stock is ...|              null|\n",
      "|16-03-2020|Me, ready to go a...|              null|\n",
      "|16-03-2020|As news of the re...|          Positive|\n",
      "|16-03-2020|\"Cashier at groce...|          Positive|\n",
      "|16-03-2020|Was at the superm...|              null|\n",
      "|16-03-2020|Due to COVID-19 o...|          Positive|\n",
      "|16-03-2020|For corona preven...|          Negative|\n",
      "|16-03-2020|All month there h...|           Neutral|\n",
      "|16-03-2020|Due to the Covid-...|              null|\n",
      "|16-03-2020|#horningsea is a ...|Extremely Positive|\n",
      "|16-03-2020|Me: I don't need ...|              null|\n",
      "|16-03-2020|ADARA Releases CO...|          Positive|\n",
      "|16-03-2020|Lines at the groc...|              null|\n",
      "|16-03-2020|????? ????? ?????...|              null|\n",
      "|16-03-2020|\"@eyeonthearctic ...|              null|\n",
      "|16-03-2020|Amazon Glitch Sty...|              null|\n",
      "|16-03-2020|For those who are...|          Positive|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12766"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()['Sentiment'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=('Sentiment'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|   TweetAt|       OriginalTweet|         Sentiment|\n",
      "+----------+--------------------+------------------+\n",
      "|16-03-2020|@MeNyrbie @Phil_G...|           Neutral|\n",
      "|16-03-2020|advice Talk to yo...|          Positive|\n",
      "|16-03-2020|Coronavirus Austr...|          Positive|\n",
      "|16-03-2020|As news of the re...|          Positive|\n",
      "|16-03-2020|\"Cashier at groce...|          Positive|\n",
      "|16-03-2020|Due to COVID-19 o...|          Positive|\n",
      "|16-03-2020|For corona preven...|          Negative|\n",
      "|16-03-2020|All month there h...|           Neutral|\n",
      "|16-03-2020|#horningsea is a ...|Extremely Positive|\n",
      "|16-03-2020|ADARA Releases CO...|          Positive|\n",
      "|16-03-2020|For those who are...|          Positive|\n",
      "|16-03-2020|with 100  nations...|Extremely Negative|\n",
      "|16-03-2020|@10DowningStreet ...|          Negative|\n",
      "|16-03-2020|UK #consumer poll...|Extremely Positive|\n",
      "|16-03-2020|In preparation fo...|          Negative|\n",
      "|16-03-2020|This morning I te...|Extremely Negative|\n",
      "|16-03-2020|There Is of in th...|          Negative|\n",
      "|16-03-2020|Went to the super...|           Neutral|\n",
      "|16-03-2020|Worried about the...|          Positive|\n",
      "|16-03-2020|my wife works ret...|          Negative|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[TweetAt: string, OriginalTweet: string, Sentiment: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"OriginalTweet\", regexp_replace(col(\"OriginalTweet\"), \"/[^0-9A-Za-z t]+/\" , \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature\n",
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Initialising the pipeline stages\n",
    "tokenizer = Tokenizer(inputCol='OriginalTweet' , outputCol='words')\n",
    "stopwords_remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
    "vectorizer = CountVectorizer(inputCol='filtered_words' , outputCol='vector_words')\n",
    "idf = IDF(inputCol='vector_words' , outputCol='vectorized_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Adding Labels\n",
    "labelEncoder = StringIndexer(inputCol='Sentiment' , outputCol='label').fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+-----+\n",
      "|   TweetAt|       OriginalTweet|         Sentiment|label|\n",
      "+----------+--------------------+------------------+-----+\n",
      "|16-03-2020|@MeNyrbie @Phil_G...|           Neutral|  2.0|\n",
      "|16-03-2020|advice Talk to yo...|          Positive|  0.0|\n",
      "|16-03-2020|Coronavirus Austr...|          Positive|  0.0|\n",
      "|16-03-2020|As news of the re...|          Positive|  0.0|\n",
      "|16-03-2020|\"Cashier at groce...|          Positive|  0.0|\n",
      "|16-03-2020|Due to COVID-19 o...|          Positive|  0.0|\n",
      "|16-03-2020|For corona preven...|          Negative|  1.0|\n",
      "|16-03-2020|All month there h...|           Neutral|  2.0|\n",
      "|16-03-2020|#horningsea is a ...|Extremely Positive|  3.0|\n",
      "|16-03-2020|ADARA Releases CO...|          Positive|  0.0|\n",
      "+----------+--------------------+------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labelEncoder.transform(df).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = labelEncoder.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+-----+\n",
      "|   TweetAt|       OriginalTweet|         Sentiment|label|\n",
      "+----------+--------------------+------------------+-----+\n",
      "|16-03-2020|@MeNyrbie @Phil_G...|           Neutral|  2.0|\n",
      "|16-03-2020|advice Talk to yo...|          Positive|  0.0|\n",
      "|16-03-2020|Coronavirus Austr...|          Positive|  0.0|\n",
      "|16-03-2020|As news of the re...|          Positive|  0.0|\n",
      "|16-03-2020|\"Cashier at groce...|          Positive|  0.0|\n",
      "|16-03-2020|Due to COVID-19 o...|          Positive|  0.0|\n",
      "|16-03-2020|For corona preven...|          Negative|  1.0|\n",
      "|16-03-2020|All month there h...|           Neutral|  2.0|\n",
      "|16-03-2020|#horningsea is a ...|Extremely Positive|  3.0|\n",
      "|16-03-2020|ADARA Releases CO...|          Positive|  0.0|\n",
      "|16-03-2020|For those who are...|          Positive|  0.0|\n",
      "|16-03-2020|with 100  nations...|Extremely Negative|  4.0|\n",
      "|16-03-2020|@10DowningStreet ...|          Negative|  1.0|\n",
      "|16-03-2020|UK #consumer poll...|Extremely Positive|  3.0|\n",
      "|16-03-2020|In preparation fo...|          Negative|  1.0|\n",
      "|16-03-2020|This morning I te...|Extremely Negative|  4.0|\n",
      "|16-03-2020|There Is of in th...|          Negative|  1.0|\n",
      "|16-03-2020|Went to the super...|           Neutral|  2.0|\n",
      "|16-03-2020|Worried about the...|          Positive|  0.0|\n",
      "|16-03-2020|my wife works ret...|          Negative|  1.0|\n",
      "+----------+--------------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Positive = 0.0\n",
    "### Negative = 1.0\n",
    "### Neutral  = 2.0\n",
    "### Extermely Positive = 3.0\n",
    "### Extremely Negative = 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = df.randomSplit([0.6,0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='vectorized_features' , labelCol='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [tokenizer, stopwords_remover, vectorizer, idf, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o154.fit.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1939)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1517)\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:826)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:226)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$Lambda$2061/295050171.apply(Unknown Source)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:223)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$Lambda$2059/645461649.apply(Unknown Source)\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:218)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$Lambda$2058/1363606530.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1343)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:218)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:103)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat org.apache.spark.ml.optim.aggregator.LogisticAggregator.<init>(LogisticAggregator.scala:194)\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$13(LogisticRegression.scala:600)\n\tat org.apache.spark.ml.classification.LogisticRegression$$Lambda$3490/596126736.apply(Unknown Source)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:58)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-be73bac61207>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o154.fit.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1939)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1517)\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:826)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:226)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$Lambda$2061/295050171.apply(Unknown Source)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:223)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$Lambda$2059/645461649.apply(Unknown Source)\n\tat org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)\n\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:218)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$Lambda$2058/1363606530.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1343)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:218)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:103)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat org.apache.spark.ml.optim.aggregator.LogisticAggregator.<init>(LogisticAggregator.scala:194)\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$13(LogisticRegression.scala:600)\n\tat org.apache.spark.ml.classification.LogisticRegression$$Lambda$3490/596126736.apply(Unknown Source)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:58)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n"
     ]
    }
   ],
   "source": [
    "lr_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
